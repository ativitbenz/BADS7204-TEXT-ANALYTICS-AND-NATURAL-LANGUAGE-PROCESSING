{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Segmentation with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.9\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library and class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf2crf in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.1.29)\n",
      "Requirement already satisfied: tensorflow>=2.1.0 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tf2crf) (2.4.1)\n",
      "Requirement already satisfied: tensorflow-addons>=0.8.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tf2crf) (0.12.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (1.1.2)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (2.4.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (3.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (0.36.2)\n",
      "Collecting numpy~=1.19.2\n",
      "  Using cached numpy-1.19.5-cp37-cp37m-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (1.32.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (1.12)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (1.12.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (0.2.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (2.10.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (1.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (2.4.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=2.1.0->tf2crf) (1.15.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (0.11.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorflow>=2.1.0->tf2crf) (3.15.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.27.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (47.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (4.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.1.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow-addons>=0.8.2->tf2crf) (2.11.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/benzativit/Library/Python/3.7/lib/python/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.4.0)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.2\n",
      "    Uninstalling numpy-1.20.2:\n",
      "      Successfully uninstalled numpy-1.20.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyldavis 3.3.1 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.20.1\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tf2crf\n",
    "# https://pypi.org/project/tf2crf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tf2crf import CRF, ModelWithCRFLoss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Text_Processor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Processor:\n",
    "    '''\n",
    "    Class for word tokenization and model training task\n",
    "    parameters:\n",
    "        chr2vec_dic_path: character embedding dictionary path\n",
    "        model_path: path of model file, model has to be trained frist if path is not provided\n",
    "        sentence_size: max size of charater per sentence\n",
    "        overlap: overlap size (overlap size should be greater than or equal to number of convolutional layers)\n",
    "    \n",
    "    attributes:\n",
    "        model: word segmentation model\n",
    "        crf: crf model flag (for new implementation)\n",
    "        sentence_size: max size of charater per sentence\n",
    "        overlap: overlap size (overlap size should be greater than or equal to number of convolutional layers)\n",
    "        ndim: inout dimension\n",
    "        look_up_dict: look up dict for character id\n",
    "        chr_size: number of character in dict\n",
    "        chr_vec_size: character embedding size\n",
    "        embedding_matrix: look up table for chracter embedding vector\n",
    "    \n",
    "    methods: **we have 2 create_model for complatibility\n",
    "        create_model_sigmoid: create new cnn word segmentation model using sigmoid\n",
    "            conv_layer: number of convolutional layers\n",
    "            conv_chanel: channel per layer\n",
    "            kernel: kernel size\n",
    "            dropout_rate: drop out rate per layer\n",
    "            \n",
    "        create_model_crf: create new cnn word segmentation model using crf\n",
    "            conv_layer: number of convolutional layers\n",
    "            conv_chanel: channel per layer\n",
    "            kernel: kernel size\n",
    "            dropout_rate: drop out rate per layer\n",
    "            glu: if True then use gate linear unit in conv layer else use relu\n",
    "            \n",
    "        save_model: save current model\n",
    "            path: model path\n",
    "            \n",
    "        load_model: load model\n",
    "            path: model path\n",
    "            \n",
    "        text_to_x_y: transform text into data for train and test\n",
    "            text: input text\n",
    "            sep: word seperator character \n",
    "        return X, Y, dim\n",
    "            \n",
    "        text_to_x: transform text into data for word segmentation task\n",
    "            text: input text\n",
    "        return X, dim\n",
    "        \n",
    "        word_tagging: tag text with word seperator\n",
    "            text: input text\n",
    "            model: word segmentation model, if model is not provided this method will use self.model\n",
    "            sep: word seperator character  \n",
    "            theshold: theshold of predicted values\n",
    "        return segmented string with word seperator\n",
    "        \n",
    "        word_tokenizer: split text into list of words\n",
    "            text: input text\n",
    "            model: word segmentation model, if model is not provided this method will use self.model\n",
    "            theshold: theshold of predicted values\n",
    "        return list of words\n",
    "        \n",
    "        get_eval: get evaluation score\n",
    "            text: input list of text\n",
    "            theshold: assignment theshold (only use if model require it)\n",
    "            model: word segmentation model, if model is not provided this method will use self.model\n",
    "        return {'char_level': {'precision', 'recall', 'f1'}, 'word_level': {'precision', 'recall', 'f1'}}\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, chr2vec_dic_path, model_path=None, crf=False, **kwargs):\n",
    "        \n",
    "        '''init parameters'''\n",
    "        self.crf = crf\n",
    "        self.sentence_size = kwargs.get('sentence_size', 100)\n",
    "        self.overlap = kwargs.get('overlap', 5)\n",
    "        self.ndim = self.sentence_size + self.overlap*2\n",
    "        \n",
    "        '''create look up dict and embedding matrix'''\n",
    "        self.look_up_dict = OrderedDict()\n",
    "        self.look_up_dict['<pad>'] = 0\n",
    "        with open(chr2vec_dic_path,'r', encoding=\"utf-8\") as f:\n",
    "            for k,v in json.load(f).items():\n",
    "                self.look_up_dict[k] = [float(i) for i in v]\n",
    "        self.chr_size = len(self.look_up_dict)\n",
    "        self.chr_vec_size = len(list(self.look_up_dict.values())[1])\n",
    "        self.look_up_dict['<pad>'] = np.zeros(self.chr_vec_size)\n",
    "        self.embedding_matrix = np.zeros((self.chr_size, self.chr_vec_size), dtype=np.float32)\n",
    "        for i, (k, v) in enumerate(self.look_up_dict.items()):\n",
    "            self.embedding_matrix[i, :] = v\n",
    "            self.look_up_dict[k] = i\n",
    "        \n",
    "        '''load model'''\n",
    "        if self.crf and model_path is not None:\n",
    "            '''for crf we have to load weight instead (bug)'''\n",
    "            self.load_model(model_path, \n",
    "                            crf = crf,\n",
    "                            conv_layer = kwargs.get('conv_layer', 5),\n",
    "                            conv_chanel = kwargs.get('conv_chanel', 100),\n",
    "                            kernel = kwargs.get('kernel', 3),\n",
    "                            dropout_rate = kwargs.get('dropout_rate', 0.2),\n",
    "                            glu = kwargs.get('glu', True)\n",
    "                           )\n",
    "        else:\n",
    "            self.model = None if model_path is None else models.load_model(model_path)\n",
    "            \n",
    "    def create_model_sigmoid(self, conv_layer=5, conv_chanel=50, kernel=3, dropout_rate=0.2):\n",
    "        \n",
    "        '''create new model and add layers'''\n",
    "        self.model = models.Sequential()\n",
    "        self.model.add(layers.InputLayer(input_shape=(self.ndim)))\n",
    "        self.model.add(layers.Embedding(self.chr_size, \n",
    "                                        self.chr_vec_size, \n",
    "                                        weights=[self.embedding_matrix], \n",
    "                                        trainable=False))\n",
    "        self.model.add(layers.Dropout(dropout_rate))\n",
    "        \n",
    "        '''create convolutional layers'''\n",
    "        for i in range(conv_layer):\n",
    "            self.model.add(layers.Conv1D(conv_chanel, \n",
    "                                         kernel, \n",
    "                                         activation='relu', \n",
    "                                         padding='same'))\n",
    "            self.model.add(layers.Dropout(dropout_rate))\n",
    "        \n",
    "        self.model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['binary_accuracy'])\n",
    "        self.crf = False\n",
    "    \n",
    "    def create_model_crf(self, conv_layer=5, conv_chanel=100, kernel=3, dropout_rate=0.2, glu = True):\n",
    "        x = layers.Input(shape=(self.ndim))\n",
    "        y = layers.Embedding(self.chr_size, \n",
    "                             self.chr_vec_size, \n",
    "                             weights=[self.embedding_matrix], \n",
    "                             trainable=False)(x)\n",
    "        y = layers.Dropout(dropout_rate)(y)\n",
    "        \n",
    "        '''create convolutional layers'''\n",
    "        if glu:\n",
    "            for i in range(conv_layer):\n",
    "                y1 = layers.Conv1D(conv_chanel, kernel, padding='same')(y)\n",
    "                y2 = layers.Conv1D(conv_chanel, kernel, activation='sigmoid', padding='same')(y)\n",
    "                y = layers.Multiply()([y1,y2])\n",
    "                y = layers.Dropout(dropout_rate)(y)\n",
    "        else:\n",
    "            y = layers.Conv1D(conv_chanel, kernel, activation='relu', padding='same')(y)\n",
    "            y = layers.Dropout(dropout_rate)(y)\n",
    "            \n",
    "        y = layers.Dense(16, activation=None)(y)\n",
    "        y = CRF(dtype='float32')(y)\n",
    "        self.model = ModelWithCRFLoss(models.Model(x, y))\n",
    "        self.model.compile(optimizer='adam')\n",
    "        self.crf = True\n",
    "        \n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)\n",
    "    \n",
    "    def load_model(self, path, crf=False, **kwargs):\n",
    "        if self.crf:\n",
    "            self.create_model_crf(conv_layer = kwargs.get('conv_layer', 5),\n",
    "                                  conv_chanel = kwargs.get('conv_chanel', 100),\n",
    "                                  kernel = kwargs.get('kernel', 3),\n",
    "                                  dropout_rate = kwargs.get('dropout_rate', 0.2),\n",
    "                                  glu = kwargs.get('glu', True)\n",
    "                                 )\n",
    "            self.model.load_weights(path+'/variables/variables')\n",
    "            self.crf = True\n",
    "        else:\n",
    "            self.model = models.load_model(path)\n",
    "            self.crf = False\n",
    "    \n",
    "    def x_y_transformer(self, text, sep='|'):\n",
    "        \n",
    "        '''assign x_id , y_values from input text'''\n",
    "        inputs_value = list()\n",
    "        outputs_value = list()\n",
    "        for word in text.split(sep):\n",
    "            if len(word) == 0:\n",
    "                continue\n",
    "            outputs_value = outputs_value + [1] + [0]*(len(word) - 1)\n",
    "            for char in word:\n",
    "                if char == '\\n':\n",
    "                    inputs_value.append(self.look_up_dict['<pad>'])\n",
    "                elif char not in self.look_up_dict.keys():\n",
    "                    inputs_value.append(self.look_up_dict['<unk>'])\n",
    "                else:\n",
    "                    inputs_value.append(self.look_up_dict[char])\n",
    "        assert len(inputs_value) == len(outputs_value), str(len(inputs_value))+' '+str(len(outputs_value))\n",
    "        \n",
    "        '''split value into sentence size list'''\n",
    "        char_size = len(outputs_value)\n",
    "        left_over = char_size%self.sentence_size\n",
    "        n_chunk = char_size//self.sentence_size\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        ret_dim = []\n",
    "        for i in range(1, n_chunk+1+(left_over>0)):\n",
    "            \n",
    "            '''add overlap values if no value before or after, add <pad> values'''\n",
    "            if i == 1:\n",
    "                inputs_before = [self.look_up_dict['<pad>']]*self.overlap\n",
    "                outputs_before = [0]*self.overlap\n",
    "            else:\n",
    "                inputs_before = inputs_value[(i-1)*self.sentence_size-self.overlap:(i-1)*self.sentence_size]\n",
    "                inputs_before = inputs_before + [self.look_up_dict['<pad>']]*(self.overlap-len(inputs_before))\n",
    "                outputs_before = outputs_value[(i-1)*self.sentence_size-self.overlap:(i-1)*self.sentence_size]\n",
    "                outputs_before = outputs_before + [0]*(self.overlap-len(outputs_before))\n",
    "            \n",
    "            if i == n_chunk+1:\n",
    "                inputs_after = [self.look_up_dict['<pad>']]*(self.sentence_size-left_over+self.overlap)\n",
    "                outputs_after = [0]*(self.sentence_size-left_over+self.overlap)\n",
    "            else:\n",
    "                inputs_after = inputs_value[i*self.sentence_size:i*self.sentence_size+self.overlap]\n",
    "                inputs_after = inputs_after + [self.look_up_dict['<pad>']]*(self.overlap-len(inputs_after))\n",
    "                outputs_after = outputs_value[i*self.sentence_size:i*self.sentence_size+self.overlap]\n",
    "                outputs_after = outputs_after + [0]*(self.overlap-len(outputs_after))\n",
    "            \n",
    "            '''combline before + sentence size char vector + after'''\n",
    "            inputs.append(inputs_before\n",
    "                          + inputs_value[(i-1)*self.sentence_size:i*self.sentence_size]\n",
    "                          + inputs_after\n",
    "                         )\n",
    "            outputs.append(outputs_before\n",
    "                          + outputs_value[(i-1)*self.sentence_size:i*self.sentence_size]\n",
    "                          + outputs_after\n",
    "                         )\n",
    "            d = self.overlap + left_over if i == n_chunk+1 else self.overlap + self.sentence_size\n",
    "            ret_dim.append((self.overlap, d))\n",
    "            \n",
    "        return inputs, outputs, ret_dim\n",
    "    \n",
    "    def x_transformer(self, text):\n",
    "        \n",
    "        '''assign x_id from input text'''\n",
    "        inputs_value = list()\n",
    "        for char in text:\n",
    "            if char == '\\n':\n",
    "                inputs_value.append(self.look_up_dict['<pad>'])\n",
    "            elif char not in self.look_up_dict.keys():\n",
    "                inputs_value.append(self.look_up_dict['<unk>'])\n",
    "            else:\n",
    "                inputs_value.append(self.look_up_dict[char])\n",
    "                \n",
    "        '''split value into sentence size list'''\n",
    "        char_size = len(inputs_value)\n",
    "        left_over = char_size%self.sentence_size\n",
    "        n_chunk = char_size//self.sentence_size\n",
    "        inputs = []\n",
    "        ret_dim = []\n",
    "        for i in range(1, n_chunk+1+(left_over>0)):\n",
    "            \n",
    "            '''add overlap values if no value before or after, add <pad> values'''\n",
    "            if i == 1:\n",
    "                inputs_before = [self.look_up_dict['<pad>']]*self.overlap\n",
    "            else:\n",
    "                inputs_before = inputs_value[(i-1)*self.sentence_size-self.overlap:(i-1)*self.sentence_size]\n",
    "                inputs_before = inputs_before + [self.look_up_dict['<pad>']]*(self.overlap-len(inputs_before))\n",
    "\n",
    "            if i == n_chunk+1:\n",
    "                inputs_after = [self.look_up_dict['<pad>']]*(self.sentence_size-left_over+self.overlap)\n",
    "            else:\n",
    "                inputs_after = inputs_value[i*self.sentence_size:i*self.sentence_size+self.overlap]\n",
    "                inputs_after = inputs_after + [self.look_up_dict['<pad>']]*(self.overlap-len(inputs_after))\n",
    "            \n",
    "            '''combline before + sentence size char vector + after'''\n",
    "            inputs.append(inputs_before\n",
    "                          + inputs_value[(i-1)*self.sentence_size:i*self.sentence_size]\n",
    "                          + inputs_after\n",
    "                         )\n",
    "            d = self.overlap + left_over if i == n_chunk+1 else self.overlap + self.sentence_size\n",
    "            s = i == n_chunk+1\n",
    "            ret_dim.append((self.overlap, d, s))\n",
    "        \n",
    "        return inputs, ret_dim\n",
    "    \n",
    "    def text_to_x_y(self, text, sep='|'):\n",
    "        X_list = []\n",
    "        Y_list = []\n",
    "        ret_dim = []\n",
    "        if type(text) is list or type(text) is tuple:\n",
    "            text = '\\n'.join(text)\n",
    "            \n",
    "        if type(text) is str and len(text.split('\\n')) > 0:\n",
    "            text = text.split('\\n')\n",
    "            for l in text:\n",
    "                l = l.strip(sep)\n",
    "                if len(l) == 0:\n",
    "                    continue\n",
    "                X, Y, r = self.x_y_transformer(l, sep)\n",
    "                X_list.extend(X)\n",
    "                Y_list.extend(Y)\n",
    "                ret_dim.extend(r)\n",
    "        elif type(text) is str:\n",
    "            X, Y, r = self.x_y_transformer(text, sep)\n",
    "            X_list.extend(X)\n",
    "            Y_list.extend(Y)\n",
    "            ret_dim.extend(r)\n",
    "        else:\n",
    "            raise AttributeError('text is not a list nor str')\n",
    "        X_list = np.array(X_list, dtype=np.int32)    \n",
    "        Y_list = np.array(Y_list, dtype=np.int32)\n",
    "        if len(X_list.shape) == 1:\n",
    "            X_list = X_list.reshape(1, X_list.shape[0])\n",
    "            Y_list = Y_list.reshape(1, Y_list.shape[0])\n",
    "        \n",
    "        return X_list, Y_list, ret_dim\n",
    "    \n",
    "    def text_to_x(self, text):\n",
    "        X_list = []\n",
    "        ret_dim = []\n",
    "            \n",
    "        if type(text) is list or type(text) is tuple:\n",
    "            for l in text:\n",
    "                X, r = self.x_transformer(l)\n",
    "                X_list.extend(X)\n",
    "                ret_dim.extend(r)\n",
    "        elif type(text) is str:\n",
    "            X, r = self.x_transformer(text)\n",
    "            X_list.extend(X)\n",
    "            ret_dim.extend(r)\n",
    "        else:\n",
    "            raise AttributeError('text is not a list nor str')\n",
    "        X_list = np.array(X_list, dtype=np.int32)    \n",
    "        if len(X_list.shape) == 1:\n",
    "            X_list = X_list.reshape(1, X_list.shape[0])\n",
    "        \n",
    "        return X_list, ret_dim\n",
    "        \n",
    "    def word_tagging(self, text, model=None, sep='|', theshold=0.5):\n",
    "        \n",
    "        '''type check'''\n",
    "        flag = False\n",
    "        if type(text) is list or type(text) is tuple:\n",
    "            flag = True\n",
    "        elif type(text) is not str:\n",
    "            raise AttributeError('this method only accept string of text')\n",
    "            \n",
    "        '''feed to model'''\n",
    "        model = self.model if model is None else model\n",
    "        X, dim = self.text_to_x(text)\n",
    "        \n",
    "        if X.shape[0] < 256 or len(tf.config.list_physical_devices('GPU')) == 0:\n",
    "            with tf.device('/cpu:0'):\n",
    "                Y = model.predict(X)[0] if self.crf else model.predict(X)\n",
    "        else:\n",
    "            with tf.device('/gpu:0'):\n",
    "                Y = model.predict(X, batch_size=1024)[0] if self.crf else model.predict(X, batch_size=1024)\n",
    "        \n",
    "        '''process word filter vector'''\n",
    "        size = X.shape[0]\n",
    "        word_filter = []\n",
    "        sent_filter = []\n",
    "        for i in range(size):\n",
    "            word_filter.extend(Y[i, dim[i][0]: dim[i][1]].flatten().tolist())\n",
    "            s = [False]*(dim[i][1]-dim[i][0])\n",
    "            s[-1] = dim[i][2]\n",
    "            sent_filter.extend(s)\n",
    "        \n",
    "        '''add saperator character'''\n",
    "        seg_text = ''\n",
    "        prev_char = ''\n",
    "        if flag:\n",
    "            text = ''.join(text)\n",
    "        for char, fil, s in zip(text, word_filter, sent_filter):\n",
    "            seg_text += sep if fil > theshold or char == '\\n' or prev_char == '\\n' else ''\n",
    "            seg_text += char\n",
    "            seg_text += '<?/cut>' if s and flag else ''\n",
    "            prev_char = char\n",
    "        \n",
    "        if flag:\n",
    "            temp = []\n",
    "            for s in seg_text.split('<?/cut>'):\n",
    "                if len(s) > 0 and s != sep:\n",
    "                    s += sep if s[-1] != sep else ''\n",
    "                    temp.append(s)\n",
    "            seg_text = temp\n",
    "        else:\n",
    "            seg_text += sep\n",
    "                    \n",
    "        return seg_text\n",
    "    \n",
    "    def word_tokenizer(self, text, model=None, theshold=0.5):\n",
    "        seg_text = self.word_tagging(text, model, '|', theshold)\n",
    "        return seg_text.strip('|').split('|')\n",
    "    \n",
    "    def get_eval(self, text, theshold=0.5, model=None, sep='|', frankenstein=False):\n",
    "        model = self.model if model is None else model\n",
    "                \n",
    "        X, Y, dim = self.text_to_x_y(text, sep=sep)\n",
    "        X = np.array(X, dtype=np.int32)\n",
    "        Y = np.array(Y, dtype=np.int32)\n",
    "        \n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(1, X.shape[0])\n",
    "            Y = Y.reshape(1, Y.shape[0])\n",
    "        \n",
    "        if X.shape[0] < 256 or len(tf.config.list_physical_devices('GPU')) == 0:\n",
    "            with tf.device('/cpu:0'):\n",
    "                Y_hat = model.predict(X)[0] if self.crf else model.predict(X)\n",
    "        else:\n",
    "            with tf.device('/gpu:0'):\n",
    "                Y_hat = model.predict(X, batch_size=1024)[0] if self.crf else model.predict(X, batch_size=1024)\n",
    "        \n",
    "        size = X.shape[0]\n",
    "        Y_true = []\n",
    "        Y_pred = []\n",
    "        for i in range(size):\n",
    "            Y_true.extend(Y[i, dim[i][0]: dim[i][1]].flatten().tolist())\n",
    "            Y_pred.extend(Y_hat[i, dim[i][0]: dim[i][1]].flatten().tolist())\n",
    "        \n",
    "        '''For doctor Frankenstein only, do not touch!'''\n",
    "        if frankenstein:\n",
    "            t = ''.join(text).replace(sep, '')\n",
    "            prev = ''\n",
    "            assert len(t) == len(Y_pred), str(len(t)) + ' ' + str(len(Y_pred))\n",
    "            for i in range(len(t)):\n",
    "                if (t[i] == '.' and prev.isnumeric()) or (t[i].isnumeric() and prev == '.'):\n",
    "                    Y_pred[i] = 0\n",
    "                elif t[i] == '%' and prev.isnumeric():\n",
    "                    Y_pred[i] = 0\n",
    "                prev = t[i]\n",
    "        \n",
    "        Y_true = np.array(Y_true, dtype=np.int32)\n",
    "        Y_pred = np.array(Y_pred, dtype=np.int32) if self.crf else np.array([i > theshold for i in Y_pred]) \n",
    "        \n",
    "\n",
    "        tp = np.sum(Y_true * Y_pred)\n",
    "        fp = np.sum((1-Y_true) * Y_pred)\n",
    "        fn = np.sum(Y_true * (1-Y_pred))\n",
    "        precision = tp / (tp+fp)\n",
    "        recall = tp / (tp+fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        \n",
    "        tp = 0\n",
    "        flag = False\n",
    "        for i,j in zip(Y_true, Y_pred):\n",
    "            if i and j:\n",
    "                tp += 1\n",
    "                flag = True\n",
    "            elif i != j and flag:\n",
    "                tp -= 1\n",
    "                flag = False\n",
    "        word_level_precision = tp/sum(Y_pred)\n",
    "        word_level_recall = tp/sum(Y_true)\n",
    "        word_level_f1 = 2 * word_level_precision * word_level_recall / (word_level_precision + word_level_recall)\n",
    "        \n",
    "        return {'char_level': {\n",
    "                    'precision': precision, \n",
    "                    'recall': recall, \n",
    "                    'f1': f1},\n",
    "                'word_level':{\n",
    "                    'precision': word_level_precision,\n",
    "                    'recall': word_level_recall,\n",
    "                    'f1': word_level_f1}\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized Text_Processor\n",
    "(choose only one model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_proc = Text_Processor('chr2vec_w10_v50.json') #empty model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_proc = Text_Processor('chr2vec_w10_v50.json','cnn_5_crf_glu_cut_poem_txt', crf=True, conv_layer=5, conv_chanel=100, kernel=3, dropout_rate=0.2, glu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and transfrom train-test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('train_cut_poem.txt','r', encoding='utf-8') as f:\n",
    "    s = f.read().split('\\n')\n",
    "    lines.extend(s)\n",
    "X_train, Y_train, _ = text_proc.text_to_x_y(lines)\n",
    "del lines\n",
    "\n",
    "lines = []\n",
    "with open('test_cut_poem.txt','r', encoding='utf-8') as f:\n",
    "    s = f.read().split('\\n')\n",
    "    lines.extend(s)\n",
    "X_test, Y_test, _ = text_proc.text_to_x_y(lines)\n",
    "del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(232153, 110), Y_train:(232153, 110)\n",
      "X_test:(58042, 110), Y_test:(58042, 110)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benzativit/Library/Python/3.7/lib/python/site-packages/tensorflow/python/autograph/impl/api.py:390: UserWarning: CRF Decoding does not work with KerasTensors in TF2.4. The bug has since been fixed in tensorflow/tensorflow##45534\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    }
   ],
   "source": [
    "text_proc.create_model_crf(conv_layer=5, conv_chanel=100, kernel=3, dropout_rate=0.2, glu=True)\n",
    "#text_proc.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "907/907 [==============================] - 556s 608ms/step - crf_loss: 24.3951 - accuracy: 0.9173 - val_crf_loss_val: 9.7458 - val_val_accuracy: 0.9661\n",
      "Epoch 2/5\n",
      "907/907 [==============================] - 553s 610ms/step - crf_loss: 10.3535 - accuracy: 0.9627 - val_crf_loss_val: 8.0402 - val_val_accuracy: 0.9716\n",
      "Epoch 3/5\n",
      "907/907 [==============================] - 511s 563ms/step - crf_loss: 8.7745 - accuracy: 0.9685 - val_crf_loss_val: 7.0478 - val_val_accuracy: 0.9752\n",
      "Epoch 4/5\n",
      "907/907 [==============================] - 534s 589ms/step - crf_loss: 7.8473 - accuracy: 0.9719 - val_crf_loss_val: 6.3125 - val_val_accuracy: 0.9780\n",
      "Epoch 5/5\n",
      "907/907 [==============================] - 512s 564ms/step - crf_loss: 7.0992 - accuracy: 0.9746 - val_crf_loss_val: 5.5951 - val_val_accuracy: 0.9808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_addons/text/crf.py:540: UserWarning: CRF Decoding does not work with KerasTensors in TF2.4. The bug has since been fixed in tensorflow/tensorflow##45534\n",
      "  \"CRF Decoding does not work with KerasTensors in TF2.4. The bug has since been fixed in tensorflow/tensorflow##45534\"\n",
      "/Users/benzativit/Library/Python/3.7/lib/python/site-packages/tensorflow/python/autograph/impl/api.py:390: UserWarning: CRF Decoding does not work with KerasTensors in TF2.4. The bug has since been fixed in tensorflow/tensorflow##45534\n",
      "  return py_builtins.overload_of(f)(*args)\n",
      "/Users/benzativit/Library/Python/3.7/lib/python/site-packages/tensorflow/python/autograph/impl/api.py:390: UserWarning: CRF Decoding does not work with KerasTensors in TF2.4. The bug has since been fixed in tensorflow/tensorflow##45534\n",
      "  return py_builtins.overload_of(f)(*args)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_addons/text/crf.py:540: UserWarning: CRF Decoding does not work with KerasTensors in TF2.4. The bug has since been fixed in tensorflow/tensorflow##45534\n",
      "  \"CRF Decoding does not work with KerasTensors in TF2.4. The bug has since been fixed in tensorflow/tensorflow##45534\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_5_crf_glu_cut_poem_txt/assets\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0' if len(tf.config.list_physical_devices('GPU')) > 0 else '/cpu:0'):\n",
    "    history = text_proc.model.fit(X_train, Y_train, batch_size=256, validation_data=(X_test, Y_test), epochs=5)\n",
    "text_proc.save_model('cnn_5_crf_glu_cut_poem_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('cnn_5_crf_glu_cut_poem_txt_acc.png', dpi=480)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['crf_loss'])\n",
    "plt.plot(history.history['val_crf_loss_val'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('cnn_5_crf_glu_cut_poem_txt_loss.png', dpi=480)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('cnn_5_crf_glu_cut_poem_txt.json', 'w') as f:\n",
    "#     json.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benzativit/Library/Python/3.7/lib/python/site-packages/tensorflow/python/autograph/impl/api.py:390: UserWarning: CRF Decoding does not work with KerasTensors in TF2.4. The bug has since been fixed in tensorflow/tensorflow##45534\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    }
   ],
   "source": [
    "text_proc = Text_Processor('chr2vec_w10_v50.json','cnn_5_crf_glu_cut_poem_txt_epoch40', crf=True, conv_layer=5, conv_chanel=100, kernel=3, dropout_rate=0.2, glu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('test_cut_poem.txt','r', encoding='utf-8') as f:\n",
    "    s = f.read().split('\\n')\n",
    "    lines.extend(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char_level': {'precision': 0.9695302984165651,\n",
       "  'recall': 0.9894761562543591,\n",
       "  'f1': 0.9794016867703618},\n",
       " 'word_level': {'precision': 0.941122937659174,\n",
       "  'recall': 0.9604843793315947,\n",
       "  'f1': 0.9507050930817249}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_proc.get_eval(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appiled model to text segmentation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_proc = Text_Processor('chr2vec_w10_v50.json','cnn_5_crf_glu_cut_poem_txt_epoch40', crf=True, conv_layer=5, conv_chanel=100, kernel=3, dropout_rate=0.2, glu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|กิน|ข้าว|ที่|ไหน|ดี|?|',\n",
       " '|ไป|กิน|ที่| |MK| |มั้ย|?| |หรือ|จะ|ไป|ร้าน|ตาม|สั่ง|']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['กินข้าวที่ไหนดี?','ไปกินที่ MK มั้ย? หรือจะไปร้านตามสั่ง']\n",
    "text_proc.word_tagging(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|ธนาคารกรุงไทย|สร้าง|ปรากฏการณ์|ครั้ง|ใหม่| |สู้|ศึก|ดิจิทัล|ใน|วงการ|ธนาคารไทย|ที่|แข่งขัน|กัน|อย่าง|ดุ|เดือด| |ด้วย|การ|เปิด|ตัว|บริษัท|ลูก| |อินฟินิธัส| |บาย| |กรุงไทย จำกัด| |(|Infinitas| |by| |Krungthai|)| |ทำ|การ|วิจัย|และ|พัฒนา|ผลิตภัณฑ์|ทาง|การ|เงิน|ดิจิทัล|รูปแบบ|ใหม่|'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'ธนาคารกรุงไทยสร้างปรากฏการณ์ครั้งใหม่ สู้ศึกดิจิทัลในวงการธนาคารไทยที่แข่งขันกันอย่างดุเดือด ด้วยการเปิดตัวบริษัทลูก อินฟินิธัส บาย กรุงไทย จำกัด (Infinitas by Krungthai) ทำการวิจัยและพัฒนาผลิตภัณฑ์ทางการเงินดิจิทัลรูปแบบใหม่'\n",
    "text_proc.word_tagging(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|ไมโครซอฟท์|ประกาศ|แยก|ส่วน|ฟีเจอร์|ใหม่|ๆ| |ของ| |Windows| |Insider| |ออก|มา|เป็น| |Windows| |Feature| |Experience| |Pack| |ต่างหาก| |เพื่อ|ปล่อย|อัพเดต|ผ่าน| |Windows| |Update| |ได้|รวดเร็ว|กว่า|เดิม| |เพราะ|ไม่|ต้อง|รอ| |Build| |ใหม่|ของ| |Insider|'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'ไมโครซอฟท์ประกาศแยกส่วนฟีเจอร์ใหม่ๆ ของ Windows Insider ออกมาเป็น Windows Feature Experience Pack ต่างหาก เพื่อปล่อยอัพเดตผ่าน Windows Update ได้รวดเร็วกว่าเดิม เพราะไม่ต้องรอ Build ใหม่ของ Insider'\n",
    "text_proc.word_tagging(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|Zoom| |รายงาน|ผล|ประกอบ|การ|ประจำ|ไตรมาส|ที่| |3| |ตาม|ปี|การ|เงิน|บริษัท| |2021| |สิ้นสุด|วัน|ที่| |31| |ตุลาคม| |2020| |ราย|ได้|รวม|เพิ่ม|ขึ้น| |367|%| |เทียบ|กับ|ช่วง|เดียว|กัน|ใน|ปี|ก่อน|เป็น| |777|.|7| |ล้าน|ดอลลาร์| |และ|มี|กำไร|สุทธิ|แบบ| |GAPP| |198|.|4| |ล้าน|ดอลลาร์|'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Zoom รายงานผลประกอบการประจำไตรมาสที่ 3 ตามปีการเงินบริษัท 2021 สิ้นสุดวันที่ 31 ตุลาคม 2020 รายได้รวมเพิ่มขึ้น 367% เทียบกับช่วงเดียวกันในปีก่อนเป็น 777.7 ล้านดอลลาร์ และมีกำไรสุทธิแบบ GAPP 198.4 ล้านดอลลาร์'\n",
    "text_proc.word_tagging(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|Although| |GLU| |turns| |out| |to| |be| |intrinsically| |simple|,| |the| |description| |of| |GLU| |from| |the| |original| |paper| |has| |been| |confusing| |to| |some| |of| |the| |readers|.| |When| |I| |worked| |on| |the| |CycleGAN| |based| |voice| |conversion|,| |I| |did| |not| |implement| |correctly| |for| |the| |first| |time|.| |After| |a| |few| |years| |when| |I| |looked| |back| |at| |the| |paper|,| |I| |almost| |misunderstood| |it| |again|.| |The| |official| |PyTorch| |GLU| |function| |was| |also| |very| |confusing| |to| |the| |users|.|'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Although GLU turns out to be intrinsically simple, the description of GLU from the original paper has been confusing to some of the readers. When I worked on the CycleGAN based voice conversion, I did not implement correctly for the first time. After a few years when I looked back at the paper, I almost misunderstood it again. The official PyTorch GLU function was also very confusing to the users.'\n",
    "text_proc.word_tagging(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|กนก|คน|ตลกชวน|ดวง|กมล|คน|ผอม|รอชมภมรดม|ดอม|ดอก|ขจร|สอง|คน|ชอบ|จอด|รถ|ตรง|ตรอก|ยอม|ทนอด|นอน|อด|กรน|รอยลภมรดม|ดอก|หอม|บน|ขอน|ตรง|คลองมอญ|ลม|บน|หวน|สอบ|จน|ปอย|ผม|ปรกคอ|สอง|สมรสมพร|คน|จร|พบ|สอง|อรชรสมพร|ปอง|สอง|สมร|ยอม|ลง|คลองลอย|คอ|มอง|สอง|อรชร|มอง|อก|มอง|คอ|มอง|ผม|มอง|จน|สอง|คน|ฉงนสมพร|บอก|ชวน|สอง|คน|ถอน|สมอ|ลง|ชลลองวอน|สอง|หน|สอง|อรชรถอย|หลบ|สมพร|วอน|จน|พล|พรรค|สด|สวย|หมด|สนกรก|นกชวน|ดวง|กมลชงนม|ผง|รอชมภมร|บนดอน|ฝน|ตก|ตลอดจน|ถนน|ปอน|จอม|ปลวก|ตรง|ตรอก|จอด|รถ|ถลอก|ปอก|ลง|สอง|สมร|มอง|นกปรอท|จ|ก|มดจก|ปลวก|จก|หนอน|ลง|คอสมพร|คง|ลอย|คอ|ลอยวน|บอก|สอ|พลอ|คน|สวย|ผสม|บท|สวด|ของ|ขอม|คน|หนอ|คน|สมพร|สวดวน|จน|อรชร|สอง|คน|ฉงน|ฉงวย|งวย|งงคอ|ตก|ยอม|นอน|ลง|บน|บก|สมพรยก|ซอง|ผง|ทอง|ปลอม|ผสม|ลง|นมชง|ของ|สอง|สมรสมพร|ถอน|ผม|นวล|ลออ|สอง|คน|ปน|ผสม|ตอน|หลอม|รวม|นม|ชงสมพร|สวด|บท|ขอม|ถอย|ว|กวน|หก|หน|ขอ|วรรค|ตอนวอน|ผอง|ชน|จงอวย|พร|สอง|ดวง|สมร|รอด|ปลอด|นรก|คน|คน|จร|หมอน|สกปรก|ฝน|ตก|จน|จอม|ปลวก|ยวบ|ลง|มด|ปลวก|หนอน|ออก|ซอกซอน|ลง|ผสม|นม|ชง|จน|บท|สวด|หมด|ผล|สมพร|คน|สกปรก|คง|หลง|ยก|นม|ชง|ซด|ลง|คอ|รอ|ครอบครอง|สอง|คน|สวย|ปลวก|มด|หนอนอลวน|ซอกซอน|จน|สมพร|ปวด|คอง|อ|ลง|หอน|นอน|ครวญ|นอน|หงอซม|บน|กอง|หนอนกอง|ปลวกรอ|หมอ|ตรวจ|ลม|ฝน|สงบ|ลง|ผอง|ปวง|ชน|พล|พรรค|ครบ|คน|ของ|สอง|อรชรยกพลสมทบ|ชก|ถอง|หวด|ตบ|สมพร|จน|ถดถอย|ตกตม|จม|ลง|คลอง|'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''กนกคนตลกชวนดวงกมลคนผอมรอชมภมรดมดอมดอกขจรสองคนชอบจอดรถตรงตรอกยอมทนอดนอนอดกรนรอยลภมรดมดอกหอมบนขอนตรงคลอง\n",
    "มอญลมบนหวนสอบจนปอยผมปรกคอสองสมรสมพรคนจรพบสองอรชรสมพรปองสองสมรยอมลงคลองลอยคอมองสองอรชรมองอกมองคอมองผมมอง\n",
    "จนสองคนฉงนสมพรบอกชวนสองคนถอนสมอลงชลลองวอนสองหนสองอรชรถอยหลบสมพรวอนจนพลพรรคสดสวยหมดสนกรกนกชวนดวงกมลชงนม\n",
    "ผงรอชมภมรบนดอนฝนตกตลอดจนถนนปอนจอมปลวกตรงตรอกจอดรถถลอกปอกลงสองสมรมองนกปรอทจกมดจกปลวกจกหนอนลงคอสมพรคงลอย\n",
    "คอลอยวนบอกสอพลอคนสวยผสมบทสวดของขอมคนหนอคนสมพรสวดวนจนอรชรสองคนฉงนฉงวยงวยงงคอตกยอมนอนลงบนบกสมพรยกซองผงทอง\n",
    "ปลอมผสมลงนมชงของสองสมรสมพรถอนผมนวลลออสองคนปนผสมตอนหลอมรวมนมชงสมพรสวดบทขอมถอยวกวนหกหนขอวรรคตอนวอนผองชนจง\n",
    "อวยพรสองดวงสมรรอดปลอดนรกคนคนจรหมอนสกปรกฝนตกจนจอมปลวกยวบลงมดปลวกหนอนออกซอกซอนลงผสมนมชงจนบทสวดหมดผลสมพร\n",
    "คนสกปรกคงหลงยกนมชงซดลงคอรอครอบครองสองคนสวยปลวกมดหนอนอลวนซอกซอนจนสมพรปวดคองอลงหอนนอนครวญนอนหงอซมบนกอง\n",
    "หนอนกองปลวกรอหมอตรวจลมฝนสงบลงผองปวงชนพลพรรคครบคนของสองอรชรยกพลสมทบชกถองหวดตบสมพรจนถดถอยตกตมจมลงคลอง'''.replace('\\n','')\n",
    "text_proc.word_tagging(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|2|,|000|.|50| |บาท|',\n",
       " '|50|.|12|%|',\n",
       " '|www.blognone.com|',\n",
       " '|https://www.blognone.com/node|/|119894|']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['2,000.50 บาท', '50.12%', 'www.blognone.com', 'https://www.blognone.com/node/119894']\n",
    "text_proc.word_tagging(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
